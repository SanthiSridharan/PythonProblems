Floating-point arithmetic
As a data analyst, you will be concerned primarily with numerical programs in which the bulk of the computational work involves floating-point computation. This notebook guides you through some of the most fundamental concepts in how computers store real numbers, so you can be smarter about your number crunching.

WYSInnWYG, or "what you see is not necessarily what you get."
One important consequence of a binary format is that when you print values in base ten, what you see may not be what you get! For instance, try running the code below.

This code invokes Python's decimal package, which implements base-10 floating-point arithmetic in software.

from decimal import Decimal
?Decimal # Asks for a help page on the Decimal() constructor
Object `Decimal # Asks for a help page on the Decimal() constructor` not found.
x = 1.0 + 2.0**(-52)

print(x)
print(Decimal(x)) # What does this do?

print(Decimal(0.1) - Decimal('0.1')) # Why does the output appear as it does?
1.0000000000000002
1.0000000000000002220446049250313080847263336181640625
5.551115123125782702118158340E-18
Aside: If you ever need true decimal storage with no loss of precision (e.g., an accounting application), turn to the decimal package. Just be warned it might be slower. See the following experiment for a practical demonstration.

from random import random

NUM_TRIALS = 2500000

print("Native arithmetic:")
A_native = [random() for _ in range(NUM_TRIALS)]
B_native = [random() for _ in range(NUM_TRIALS)]
%timeit [a+b for a, b in zip(A_native, B_native)]

print("\nDecimal package:")
A_decimal = [Decimal(a) for a in A_native]
B_decimal = [Decimal(b) for b in B_native]
%timeit [a+b for a, b in zip(A_decimal, B_decimal)]
Native arithmetic:
177 ms ¬± 481 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)

Decimal package:
576 ms ¬± 678 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)
The same and not the same. Consider the following two program fragments:

Program 1:

s = a - b
t = s + b

Program 2:

s = a + b
t = s - b

Let  a=1.0  and  b=œµd/2‚âà1.11√ó10‚àí16 , i.e., machine epsilon for IEEE-754 double-precision. Recall that we do not expect these programs to return the same value; let's run some Python code to see.

Note: The IEEE standard guarantees that given two finite-precision floating-point values, the result of applying any binary operator to them is the same as if the operator were applied in infinite-precision and then rounded back to finite-precision. The precise nature of rounding can be controlled by so-called rounding modes; the default rounding mode is "round-half-to-even."

a = 1.0
b = 2.**(-53) # == $\epsilon_d$ / 2.0

s1 =  a - b
t1 = s1 + b

s2 =  a + b
t2 = s2 - b

print("s1:", s1.hex())
print("t1:", t1.hex())
print("\n")
print("s2:", s2.hex())
print("t2:", t2.hex())

print("")
print(t1, "vs.", t2)
print("(t1 == t2) == {}".format(t1 == t2))
s1: 0x1.fffffffffffffp-1
t1: 0x1.0000000000000p+0


s2: 0x1.0000000000000p+0
t2: 0x1.fffffffffffffp-1

1.0 vs. 0.9999999999999999
(t1 == t2) == False
By the way, the NumPy/SciPy package, which we will cover later in the semester, allows you to determine machine epsilon in a portable way. Just note this fact for now.

Here is an example of printing machine epsilon for both single-precision and double-precision values.

import numpy as np

EPS_S = np.finfo (np.float32).eps
EPS_D = np.finfo (float).eps

print("Single-precision machine epsilon:", float(EPS_S).hex(), "~", EPS_S)
print("Double-precision machine epsilon:", float(EPS_D).hex(), "~", EPS_D)
Single-precision machine epsilon: 0x1.0000000000000p-23 ~ 1.1920929e-07
Double-precision machine epsilon: 0x1.0000000000000p-52 ~ 2.220446049250313e-16
Analyzing floating-point programs
Let's say someone devises an algorithm to compute  f(x) . For a given value  x , let's suppose this algorithm produces the value  alg(x) . One important question might be, is that output "good" or "bad?"

Forward stability. One way to show that the algorithm is good is to show that

‚à£‚à£alg(x)‚àíf(x)‚à£‚à£
 
is "small" for all  x  of interest to your application. What is small depends on context. In any case, if you can show it then you can claim that the algorithm is forward stable.

Backward stability. Sometimes it is not easy to show forward stability directly. In such cases, you can also try a different technique, which is to show that the algorithm is, instead, backward stable.

In particular,  alg(x)  is a backward stable algorithm to compute  f(x)  if, for all  x , there exists a "small"  Œîx  such that

alg(x)=f(x+Œîx).
 
In other words, if you can show that the algorithm produces the exact answer to a slightly different input problem (meaning  Œîx  is small, again in a context-dependent sense), then you can claim that the algorithm is backward stable.

Round-off errors. You already know that numerical values can only be represented finitely, which introduces round-off error. Thus, at the very least we should hope that a scheme to compute  f(x)  is as insensitive to round-off errors as possible. In other words, given that there will be round-off errors, if you can prove that  alg(x)  is either forward or backward stable, then that will give you some measure of confidence that your algorithm is good.

Here is the "standard model" of round-off error. Start by assuming that every scalar floating-point operation incurs some bounded error. That is, let  a‚äôb  be the exact mathematical result of some operation on the inputs,  a  and  b , and let  fl(a‚äôb)  be the computed value, after rounding in finite-precision. The standard model says that

fl(a‚äôb)‚â°(a‚äôb)(1+Œ¥),
 
where  |Œ¥|‚â§œµ , machine epsilon.

Let's apply these concepts on an example.

Example: Computing a sum
Let  x‚â°(x0,‚Ä¶,xn‚àí1)  be a collection of input data values. Suppose we wish to compute their sum.

The exact mathematical result is

f(x)‚â°‚àëi=0n‚àí1xi.
 
Given  x , let's also denote its exact sum by the synonym  sn‚àí1‚â°f(x) .

Now consider the following Python program to compute its sum:

def alg_sum(x): # x == x[:n]
    s = 0.
    for x_i in x: # x_0, x_1, \ldots, x_{n-1}
        s += x_i
    return s
In exact arithmetic, meaning without any rounding errors, this program would compute the exact sum. (See also the note below.) However, you know that finite arithmetic means there will be some rounding error after each addition.

Let  Œ¥i  denote the (unknown) error at iteration  i . Then, assuming the collection x represents the input values exactly, you can show that alg_sum(x) computes  sÃÇ n‚àí1  where

sÃÇ n‚àí1‚âàsn‚àí1+‚àëi=0n‚àí1siŒ¥i,
 
that is, the exact sum plus a perturbation, which is the second term (the sum). The question, then, is under what conditions will this sum will be small?

Using a backward error analysis, you can show that

sÃÇ n‚àí1‚âà‚àëi=0n‚àí1xi(1+Œîi)=f(x+Œî),
 
where  Œî‚â°(Œî0,Œî1,‚Ä¶,Œîn‚àí1) . In other words, the computed sum is the exact solution to a slightly different problem,  x+Œî .

To complete the analysis, you can at last show that

|Œîi|‚â§(n‚àíi)œµ,
 
where  œµ  is machine precision. Thus, as long as  nœµ‚â™1 , then the algorithm is backward stable and you should expect the computed result to be close to the true result. Interpreted differently, as long as you are summing  n‚â™1œµ  values, then you needn't worry about the accuracy of the computed result compared to the true result:

print("Single-precision: 1/epsilon_s ~= {:.1f} million".format(1e-6 / EPS_S))
print("Double-precision: 1/epsilon_d ~= {:.1f} quadrillion".format(1e-15 / EPS_D))
Single-precision: 1/epsilon_s ~= 8.4 million
Double-precision: 1/epsilon_d ~= 4.5 quadrillion
Based on this result, you can probably surmise why double-precision is usually the default in many languages.

In the case of this summation, we can quantify not just the backward error (i.e.,  Œîi ) but also the forward error. In that case, it turns out that

‚à£‚à£sÃÇ n‚àí1‚àísn‚àí1‚à£‚à£‚™Önœµ‚Äñx‚Äñ1.
 
Note: Analysis in exact arithmetic. We claimed above that alg_sum() is correct in exact arithmetic, i.e., in the absence of round-off error. You probably have a good sense of that just reading the code.

However, if you wanted to argue about its correctness more formally, you might do so as follows using the technique of proof by induction. When your loops are more complicated and you want to prove that they are correct, you can often adapt this technique to your problem.

First, assume that the for loop enumerates each element p[i] in order from i=0 to n-1, where n=len(p). That is, assume p_i is p[i].

Let  pk‚â°ùöô[k]  be the  k -th element of p[:]. Let  si‚â°‚àëik=0pk ; in other words,  si  is the exact mathematical sum of p[:i+1]. Thus,  sn‚àí1  is the exact sum of p[:].

Let  sÃÇ ‚àí1  denote the initial value of the variable s, which is 0. For any  i‚â•0 , let  sÃÇ i  denote the computed value of the variable s immediately after the execution of line 4, where  i=ùöí . When  i=ùöí=0 ,  sÃÇ 0=sÃÇ ‚àí1+p0=p0 , which is the exact sum of p[:1]. Thus,  sÃÇ 0=s0 .

Now suppose that  sÃÇ i‚àí1=si‚àí1 . When  ùöí=i , we want to show that  sÃÇ i=si . After line 4 executes,  sÃÇ i=sÃÇ i‚àí1+pi=si‚àí1+pi=si . Thus, the computed value  sÃÇ i  is the exact sum  si .

If  i=n , then, at line 5, the value  ùöú=sÃÇ n‚àí1=sn‚àí1 , and thus the program must in line 5 return the exact sum.

A numerical experiment: Summation
Let's do an experiment to verify that these bounds hold.

Exercise 0 (2 points). In the code cell below, we've defined a list,

N = [10, 100, 1000, 10000, 100000, 1000000, 10000000]
Take each entry N[i] to be a problem size.
Let t[:len(N)] be a list, which will hold computed sums.
For each N[i], run an experiment where you sum a list of values x[:N[i]] using alg_sum(). You should initialize x[:] so that all elements have the value 0.1. Store the computed sum in t[i].
N = [10, 100, 1000, 10000, 100000, 1000000, 10000000]

# Initialize an array t of size len(N) to all zeroes.
t = [0.0] * len(N)  

# Your code should do the experiment described above for
# each problem size N[i], and store the computed sum in t[i].
t = [ alg_sum([0.1] * item) for item in N]

print(t)

# Test: `experiment_results`
import pandas as pd
from IPython.display import display

import matplotlib.pyplot as plt
%matplotlib inline

s = [1., 10., 100., 1000., 10000., 100000., 1000000.] # exact sums
t_minus_s_rel = [(t_i - s_i) / s_i for s_i, t_i in zip (s, t)]
rel_err_computed = [abs(r) for r in t_minus_s_rel]
rel_err_bound = [ni*EPS_D for ni in N]

# Plot of the relative error bound
plt.loglog (N, rel_err_computed, 'b*', N, rel_err_bound, 'r--')

print("Relative errors in the computed result:")
display (pd.DataFrame ({'n': N, 'rel_err': rel_err_computed, 'rel_err_bound': [n*EPS_D for n in N]}))


print(assert all([abs(r) <= n*EPS_D for r, n in zip(t_minus_s_rel, N)]))

print("\n(Passed!)")
  File "<ipython-input-1-772e38836950>", line 32
    print(assert all([abs(r) <= n*EPS_D for r, n in zip(t_minus_s_rel, N)]))
               ^
SyntaxError: invalid syntax
# Test: `experiment_results`
import pandas as pd
from IPython.display import display

import matplotlib.pyplot as plt
%matplotlib inline

s = [1., 10., 100., 1000., 10000., 100000., 1000000.] # exact sums
t_minus_s_rel = [(t_i - s_i) / s_i for s_i, t_i in zip (s, t)]
rel_err_computed = [abs(r) for r in t_minus_s_rel]
rel_err_bound = [ni*EPS_D for ni in N]

# Plot of the relative error bound
plt.loglog (N, rel_err_computed, 'b*', N, rel_err_bound, 'r--')

print("Relative errors in the computed result:")
display (pd.DataFrame ({'n': N, 'rel_err': rel_err_computed, 'rel_err_bound': [n*EPS_D for n in N]}))

assert all([abs(r) <= n*EPS_D for r, n in zip(t_minus_s_rel, N)])

print("\n(Passed!)")
Relative errors in the computed result:
n	rel_err	rel_err_bound
0	10	1.00000	2.220446e-15
1	100	0.00000	2.220446e-14
2	1000	0.10000	2.220446e-13
3	10000	0.11000	2.220446e-12
4	100000	0.11100	2.220446e-11
5	1000000	0.11110	2.220446e-10
6	10000000	0.11111	2.220446e-09
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-22-7fff081a0834> in <module>
     17 display (pd.DataFrame ({'n': N, 'rel_err': rel_err_computed, 'rel_err_bound': [n*EPS_D for n in N]}))
     18 
---> 19 assert all([abs(r) <= n*EPS_D for r, n in zip(t_minus_s_rel, N)])
     20 
     21 print("\n(Passed!)")

AssertionError: 

Computing dot products
Let  x  and  y  be two vectors of length  n , and denote their dot product by  f(x,y)‚â°xTy .

Now suppose we store the values of  x  and  y  exactly in two Python arrays, x[0:n] and y[0:n]. Further suppose we compute their dot product by the program, alg_dot().

def alg_dot (x, y):
    p = [xi*yi for (xi, yi) in zip (x, y)]
    s = alg_sum (p)
    return s
Exercise 1 (OPTIONAL -- 0 points, not graded or collected). Show under what conditions alg_dot() is backward stable.

Hint. Let  (xk,yk)  denote the exact values of the corresponding inputs,  (ùö°[k],ùö¢[k]) . Then the true dot product,  xTy=‚àën‚àí1l=0xlyl . Next, let  pÃÇ k  denote the  k -th computed product, i.e.,  pÃÇ k‚â°xkyk(1+Œ≥k) , where  Œ≥k  is the  k -th round-off error and  |Œ≥k|‚â§œµ . Then apply the results for alg_sum() to analyze alg_dot().

Answer. Following the hint, alg_sum will compute  sÃÇ n‚àí1  on the computed inputs,  {pÃÇ k} . Thus,

sÃÇ n‚àí1‚âà==‚àël=0n‚àí1pÃÇ l(1+Œîl)‚àël=0n‚àí1xlyl(1+Œ≥l)(1+Œîl)‚àël=0n‚àí1xlyl(1+Œ≥l+Œîl+Œ≥lŒîl).
 
Mathematically, this appears to be the exact dot product to an input in which  x  is exact and  y  is perturbed (or vice-versa). To argue that alg_dot is backward stable, we need to establish under what conditions the perturbation,  ‚à£‚à£Œ≥l+Œîl+Œ≥lŒîl‚à£‚à£ , is "small." Since  |Œ≥l|‚â§œµ  and  |Œîl|‚â§nœµ ,

‚à£‚à£Œ≥l+Œîl+Œ≥lŒîl‚à£‚à£‚â§|Œ≥l|+|Œîl|+|Œ≥l|‚ãÖ|Œîl|‚â§(n+1)œµ+Óàª(nœµ2)‚âà(n+1)œµ.
 
More accurate summation
Suppose you wish to compute the sum,  s=x0+x1+x2+x3 . Let's say you use the "standard algorithm," which accumulates the terms one-by-one from left-to-right, as done by alg_sum() above.

For the standard algorithm, let the  i -th addition incur a roundoff error,  Œ¥i . Then our usual error analysis would reveal that the absolute error in the computed sum,  sÃÇ  , is approximately:

sÃÇ ‚àís‚âàx0(Œ¥0+Œ¥1+Œ¥2+Œ¥3)+x1(Œ¥1+Œ¥2+Œ¥3)+x2(Œ¥2+Œ¥3)+x3Œ¥3.
 
And since  |Œ¥i|‚â§œµ , you would bound the absolute value of the error by,

‚à£‚à£sÃÇ ‚àís‚à£‚à£‚â≤(4|x0|+3|x1|+2|x2|+1|x3|)œµ.
 
Notice that  |x0|  is multiplied by 4,  |x1|  by 3, and so on.

In general, if there are  n  values to sum, the  |xi|  term will be multiplied by  n‚àíi .

Exercise 2 (3 points). Based on the preceding observation, implement a new summation function, alg_sum_accurate(x) that computes a more accurate sum than alg_sum().

Hint 1. You do not need Decimal() in this problem. Some of you will try to use it, but it's not necessary. In fact, using it is likely to generate "out-of-memory" errors.

Hint 2. Some of you will try to "implement" the error formula to somehow compensate for the round-off error. But that shouldn't make sense to do. (Why not? Because the formula above is a bound, not an exact formula.) Instead, the intent of this problem is to see if you can look at the formula and understand how to interpret it. That is, how does the formula tell you to perform the summation to make it more accurate than the conventional method?

Hint 3. Some of you will look up fancy algorithms to compensate for error. That's great, because you may learn something deep in the process, but it's also not necessary. Again, see Hint 2.

Note. The test cell will disallow certain "shortcuts" on this problem, namely, importing certain implementations of summation. Only basic Python is needed.

def alg_sum_accurate(x):
    assert type(x) is list
    ###
    ### YOUR CODE HERE
    ###
# Test: `alg_sum_accurate_test`
from math import exp
from numpy.random import lognormal

# Disallow certain implementations:
import numpy
if 'sum' in dir(numpy): del numpy.sum

print("Generating non-uniform random values...")
N = [10, 10000, 10000000]
x = [lognormal(-10.0, 10.0) for _ in range(max(N))]
print("Range of input values: [{}, {}]".format(min(x), max(x)))

print("Computing the 'exact' sum. May be slow so please wait...")
x_exact = [Decimal(x_i) for x_i in x]
s_exact = [float(sum(x_exact[:n])) for n in N]
print("==>", s_exact)

print("Running alg_sum()...")
s_alg = [alg_sum(x[:n]) for n in N]
print("==>", s_alg)

print("Running alg_sum_accurate()...")
s_acc = [alg_sum_accurate(x[:n]) for n in N]
print("==>", s_acc)

print("Summary of relative errors:")
ds_alg = [abs(s_a - s_e) / s_e for s_a, s_e in zip(s_alg, s_exact)]
ds_acc = [abs(s_a - s_e) / s_e for s_a, s_e in zip(s_acc, s_exact)]
display (pd.DataFrame ({'n': N,
                        'rel_err(alg_sum)': ds_alg,
                        'rel_err(alg_sum_accurate)': ds_acc}))

assert all([r_acc < r_alg for r_acc, r_alg in zip(ds_acc[1:], ds_alg[1:])]), \
       "The 'accurate' algorithm appears to be less accurate than the conventional one!"

print("\n(Passed!)")
